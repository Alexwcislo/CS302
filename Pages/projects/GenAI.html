<h4 class="text-primary mb-3">GenAI x Epistemology</h4>
<p> Building a personalized AI assistant was an interesting assignment. Given the starter code for the AI the biggest choice was deciding the system instruction to prompt it with. I chose to make it respond in rhyming sentences/phrases. I thought this was a wholesome use as it accurately (or as accurately as the gemini model was) answers the question in a poem like way.
</p>

<h5 class="text-primary mb-3">CLI AI Assistant</h5>
<p> Once I have activated my virtual environment and in the terminal type python3 cli_assistant.py, I am able to have the followin interaction with my AI assistant.</p>
<img class="w-100 img-fluid mb-4" src="/CS302/img/cliai.png" alt="CLI AI assistnant interaction">

<p> The purpose of the CLI assistant was to ensure working code and that the AI responses were as I intended before migrating to a web (flask) based AI assistant. The system instruction is in a way the core to the AI assistant as it defines the epistemology, tone, behavior, and constraints. I was going for a playful AI assistant but from an epistemological perspective on what is knowledge my AI assistant outputs everything as if it were right. For instance, I could've prompted it to admit when it is unsure of a fact or piece of information but I didn't.
</p>

<h5 class="text-primary mb-3">Web Based AI Assitant</h5>
<p> The image below shows the web based implentation. The core of the AI assistant is the same the only difference is interactions are done through the form on the page as opposed to in the terminal.</p>
<img class="w-100 img-fluid mb-4" src="/CS302/img/webai.png" alt="Web AI assistant implentation">

<p> Similarly to above this implementation was kept simple. There are a lot of things to keep in mind when deciding how to deisign a more complicated knowledge such as if your falsley advertising the capabalities of the AI. I think extremely polished interfaces risk overstating the capabilities of the AI and falsely advertising it as having a deeper understanding than it does. Also, I felt keeping the design of the page simple gives more emphasis on the fact that the purpose of the bot is to rhyme whenever it answers a question.
</p>


<h5 class="text-primary mb-3">Reflection on the nature, origin, and limits of knowledge within GenAI:</h5>
<p> During the past couple weeks in class we discussed whether genAI and LLMs can truly posses knowledge or whether their outputs is merely a simualtion/halucination of knowledge. These discussions somewhat changed my prior thinking.</p>

<p> One big topic of conversation was the distinction or lack of between knowledge and instinct. I have always believed instinct is an biological response to stimuli whereas knowledge is a justified and true belief to a person/group of people. I also think knowledge is a construct formed by acquired information and lived exeperiences. During on discussion, Hudson suggested that turtles have knowledge because they instinctively move toward the sea when they hatch. Someone else followed up by saying babies cry when they get hungry. These exampls challenged my initial beliefs and I began to refine my thoughts. I started thinking instinct alone is not knowledge but can become knowledge through repeated experiences. For example, a baby initially cries because of some unknown feeling of discomfort and then gets fed by their parents. After repeated exposure to this feeling, crying, and being fed, the baby begins to associate the sensation to food and start grasping/gaining the knowledge of what hunger is rather than considering hunger as an instinct</p>

<p>When reading a summary of Plato's Allegory of the Cave I thought if the prisoners have only ever seen shadows, and they make up their entire reality then within that framework their beliefs count as knowledge purely based on the lack of access to a broader truth or experience. When one escapes and sees the world as it truly is he has gained knowledge and a new understading of what everything means. He is unable to transfer this knowledge to others as knowledge is something that requires experience. Along these lines LLMs do not have these worldy experiences but can only create a fabrication of it. It predicts based on patterns in training data and thus can not have knowledge at least in the same way as humans. As mentioned in 'Modern-Day Oracles or Bullshit Machines?' outputs consistent with moral reasoning does not mean the system is reasoning morally. It merely mirrors patterns it has absorbed. This simulation of knowledge does not equal actual knowledge even though the responses are reliable.</p>

<h6 class="text-primary mb-3">Lingering Questions</h5>
<p>
    These discussions and readings have left me with a deeper understanding of epistemology and GenAI however, I am left with a couple questions:
</p>
<ul>
    <li>
        Are we (me in this discussion and the broader class) being too human-centered when defining and discussing knowledge?
    </li>
    <li>
       If experience is necessary for knowledge, could an embodiment of AI meet that criteria. Or is conciousness also necessary as opposed to functional performance?
    </li>
</ul>

<h6 class="text-primary md-3"> Works Cited: </h6>
<p>Bergstrom, C. T., & West, J. D. (2025). Modernâ€‘day oracles or bullshit machines? How to thrive in a ChatGPT world. Online course. https://thebullshitmachines.com</p>
<p>Salac, J. Allegory of The Cave Prep. Source: Philosophy Learning & Teaching Organization</p>
